# InfoMetis v0.5.0 Quick Start Guide

**Version**: v0.5.0 - Kafka Ecosystem Platform  
**Estimated Time**: 15-20 minutes  
**Target Audience**: New team members, data engineers, streaming analytics developers

## Prerequisites Checklist

Before starting, ensure you have:

### ✅ System Requirements
- **OS**: Linux (Ubuntu 20.04+ recommended) or WSL2
- **Memory**: 16GB RAM minimum (complete Kafka ecosystem)
- **Storage**: 25GB free space (containers + persistent data)
- **Network**: Internet access for initial setup

### ✅ Required Tools
```bash
# Install Docker (for container runtime)
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
# Log out and log back in

# Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Install Node.js (required for interactive console)
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Verify installations
docker --version
kubectl version --client
node --version
```

## Quick Deployment (15 minutes)

### Step 1: Get InfoMetis v0.5.0
```bash
# Clone repository
git clone https://github.com/infometish/InfoMetis.git
cd InfoMetis

# Navigate to v0.5.0 release
cd v0.5.0
```

### Step 2: Deploy Complete Platform
```bash
# Start the interactive console
node console.js

# Follow the guided deployment:
# 1. Press 'K' (Kubernetes Cluster) → 'a' (auto-execute all)
# 2. Press 'D' (Deployments) → 'a' (deploy everything)
# 3. Wait ~15 minutes for complete deployment

# Alternative: Quick deployment script
echo "Deploying InfoMetis v0.5.0 Kafka Ecosystem..."
node console.js # Then: K → a, D → a
```

### Step 3: Verify Complete Deployment
```bash
# Check all pods are running (should see 15+ pods)
kubectl get pods -n infometis

# Expected services:
# - NiFi (data pipelines)
# - Kafka (event streaming)
# - Flink (stream processing)
# - ksqlDB (streaming SQL)
# - Elasticsearch (search/analytics)
# - Grafana (dashboards)
# - Prometheus (monitoring)
# - Schema Registry (schema management)
```

**✅ Success Indicators**:
- All pods show `Running` status
- Services accessible via web interfaces
- Complete Kafka ecosystem deployed

## Platform Access Points

### Web Interfaces
| Service | URL | Credentials |
|---------|-----|-------------|
| **NiFi** | http://localhost/nifi | admin / infometis2024 |
| **Kafka UI** | http://localhost/kafka-ui | - |
| **Flink UI** | http://localhost:8083 | - |
| **Grafana** | http://localhost/grafana | admin / admin |
| **Elasticsearch** | http://localhost/elasticsearch | - |
| **Prometheus** | http://localhost/prometheus | - |
| **Traefik Dashboard** | http://localhost:8082 | - |

### Command Line Interfaces
```bash
# ksqlDB CLI
kubectl exec -it -n infometis deployment/ksqldb-server -- ksql

# Kafka CLI
kubectl exec -it -n infometis kafka-0 -- bash

# Flink CLI
kubectl exec -it -n infometis deployment/flink-jobmanager -- bash
```

## Understanding the Platform

### What Just Happened?

1. **k0s Kubernetes Cluster**: Lightweight Kubernetes running the entire ecosystem
2. **Traefik Ingress**: Unified access point for all web services
3. **Kafka Ecosystem**: Complete streaming analytics platform
4. **Persistent Storage**: 70GB+ allocated across all components
5. **Service Mesh**: Internal communication between all components

### Complete Architecture
```
External Access → Traefik Ingress → [NiFi, Kafka UI, Flink, Grafana, etc.]
                       ↓
            k0s Kubernetes Cluster
                       ↓
    [NiFi] → [Kafka] → [Flink/ksqlDB] → [Elasticsearch] → [Grafana]
              ↓              ↓
        [Schema Registry] [Prometheus]
                       ↓
              Persistent Storage (70GB+)
```

## First Steps with the Platform

### 1. Create Your First Kafka Topic
```bash
kubectl exec -it -n infometis kafka-0 -- \
  kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic my-first-topic \
  --partitions 3
```

### 2. Build a Simple Data Pipeline in NiFi
1. Access NiFi: http://localhost/nifi (admin/infometis2024)
2. Drag **GenerateFlowFile** processor
3. Drag **PublishKafka** processor
4. Configure PublishKafka:
   - Kafka Brokers: `kafka-service:9092`
   - Topic Name: `my-first-topic`
5. Connect and start the flow

### 3. Process Streams with ksqlDB
```bash
# Access ksqlDB CLI
kubectl exec -it -n infometis deployment/ksqldb-server -- ksql

# Create a stream
CREATE STREAM my_stream (
  id VARCHAR,
  message VARCHAR
) WITH (
  KAFKA_TOPIC='my-first-topic',
  VALUE_FORMAT='JSON'
);

# Query the stream
SELECT * FROM my_stream EMIT CHANGES;
```

### 4. Submit a Flink Job
```bash
# Access Flink UI: http://localhost:8083
# Or submit via CLI
kubectl exec -it -n infometis deployment/flink-jobmanager -- \
  flink run /opt/flink/examples/streaming/WordCount.jar
```

### 5. Monitor with Prometheus and Grafana
1. Prometheus: http://localhost/prometheus
2. Grafana: http://localhost/grafana (admin/admin)
3. Add Prometheus data source: `http://prometheus-server-service:9090`

## Common Operations

### Platform Management
```bash
# Check platform health
kubectl get pods -n infometis

# Platform logs
kubectl logs -n infometis deployment/kafka --tail=50

# Storage usage
kubectl get pvc -n infometis

# Clean up (when needed)
node console.js # Then: R → a (remove all), K → 1 (teardown)
```

### Kafka Operations
```bash
# List topics
kubectl exec -it -n infometis kafka-0 -- \
  kafka-topics.sh --list --bootstrap-server localhost:9092

# Produce messages
kubectl exec -it -n infometis kafka-0 -- \
  kafka-console-producer.sh \
  --broker-list localhost:9092 \
  --topic my-first-topic

# Consume messages
kubectl exec -it -n infometis kafka-0 -- \
  kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-first-topic \
  --from-beginning
```

### Stream Processing
```bash
# Flink job management
kubectl exec -it -n infometis deployment/flink-jobmanager -- \
  flink list

# ksqlDB queries
kubectl exec -it -n infometis deployment/ksqldb-server -- \
  ksql -e "SHOW STREAMS;"
```

## Troubleshooting

### Common Issues

1. **Pods Pending**: Usually PVC binding issues
   ```bash
   kubectl get pv  # Check persistent volumes
   kubectl describe pvc -n infometis  # Check claims
   ```

2. **Out of Memory**: Increase system RAM or reduce component resources
   ```bash
   kubectl top nodes  # Check resource usage
   kubectl top pods -n infometis
   ```

3. **Service Not Accessible**: Check Traefik and ingress
   ```bash
   kubectl get svc -n infometis
   kubectl get ingress -n infometis
   ```

### Get Help
- **Comprehensive Docs**: `/v0.5.0/docs/PROTOTYPING_GUIDE.md` (30+ tutorials)
- **Component Guides**: `/v0.5.0/docs/components/` (Kafka, NiFi, Flink)
- **Architecture**: `/v0.5.0/docs/ARCHITECTURE.md` (system design)

## Next Steps

### Learning Path
1. **Complete Prototyping Guide**: Follow hands-on tutorials
2. **Build End-to-End Pipeline**: API → NiFi → Kafka → Flink → Elasticsearch
3. **Create Dashboards**: Build monitoring with Grafana
4. **Advanced Patterns**: Complex event processing, stream joins

### Production Considerations
1. **Security**: Enable authentication and TLS
2. **Scaling**: Horizontal scaling patterns
3. **Monitoring**: Advanced alerting with Prometheus
4. **Backup**: Data persistence strategies

## Success Criteria

After completing this guide, you should be able to:
- [ ] Deploy complete Kafka ecosystem in <20 minutes
- [ ] Access all 8 web interfaces
- [ ] Create Kafka topics and process messages
- [ ] Build data pipelines with NiFi
- [ ] Process streams with Flink and ksqlDB
- [ ] Monitor platform with Prometheus/Grafana
- [ ] Troubleshoot common deployment issues

---

*InfoMetis v0.5.0 provides a complete Kafka ecosystem platform for rapid prototyping of streaming analytics solutions. For comprehensive documentation and advanced tutorials, see the complete documentation in `/v0.5.0/docs/`.*